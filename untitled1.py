# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wA2rspysAsGncf2w2dRc321WzVFA2s7-
"""

#Importing Liabraries
import wordcloud
import nltk
import pandas as pd
from nltk.corpus import stopwords
from textblob import Word
from sklearn.preprocessing import LabelEncoder
from collections import Counter
import wordcloud
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
from keras.models import Sequential
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

#Loading the dataset
from google.colab import files
uploaded = files.upload()

import io
data = pd.read_csv(io.BytesIO(uploaded['amazon_alexa.tsv']), sep='\t')

#Creating a new column 'sentiment' based on 'rating'
def sentiments(df):
  if df['rating']> 3.0:
    return 'positive'
  elif df['rating'] <= 3.0:
    return 'negative'
data['sentiment'] = data.apply(sentiments, axis = 1)

data.head(10)

#Checking for null values in the dataset
data_v1 = data[['verified_reviews', 'sentiment']]
data_v1
data_v1.isnull().sum()

data_v1

import nltk
nltk.download('stopwords')

"""**Clearing the data. It includes removing the special characters, digits, symbols and stop words.**"""

import nltk
nltk.download('wordnet')

def cleaning(df, stop_words):

    df['verified_reviews'] = df['verified_reviews'].apply(lambda x: ' '.join(x.lower() for x in x.split()))


    # Replacing the digits/numbers

    df['verified_reviews'] = df['verified_reviews'].str.replace('d', '')

    # Removing stop words

    df['verified_reviews'] = df['verified_reviews'].apply(lambda x: ' '.join(x for x in x.split() if x not in stop_words))

    # Lemmatization

    df['verified_reviews'] = df['verified_reviews'].apply(lambda x: ' '.join([Word(x).lemmatize() for x in x.split()]))

    return df

stop_words = stopwords.words('english')

data_v1 = cleaning(data_v1, stop_words)

data_v1

common_words=''
for i in data_v1.verified_reviews:
    i = str(i)
    tokens = i.split()
    common_words += " ".join(tokens)+" "
wordcloud = wordcloud.WordCloud().generate(common_words)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

# Encoded the target column
lb = LabelEncoder()
data_v1['sentiment'] = lb.fit_transform(data_v1['sentiment'])

from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D

"""**Tokenizing and converting the reviews into numerical vactors**"""

tokenizer = Tokenizer(num_words=500, split = ' ')
tokenizer.fit_on_texts(data_v1['verified_reviews'].values)
X = tokenizer.texts_to_sequences(data_v1['verified_reviews'].values)
X = pad_sequences(X)

"""**Num_words:** This hyperparameter refers to the number of words to keep based on the frequency of the words.
**Split:** This hyper parameter reffers to seperator used for spliting the word.
**pad_sequence():** function is used to convert a list of sequence into 2D numpy array.

**Building the LSTM model using the ‘Keras’ library. This step involves model initialization, adding required LSTM layers, and model compilation**
"""

model = Sequential()
model.add(Embedding(500, 120, input_length = X.shape[1]))
model.add(SpatialDropout1D(0.4))
model.add(LSTM(176, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(2,activation='softmax'))
model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])
print(model.summary())

"""**Splitting the data into training and testing data.**"""

#Splitting the data into training and testing
y=pd.get_dummies(data_v1['sentiment'])
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 42)

"""**Training the model using training data.**"""

batch_size=32
model.fit(X_train, y_train, epochs = 5, batch_size=batch_size, verbose = 'auto')

"""**Evaluating the model.**"""

model.evaluate(X_test,y_test)

